{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __NLP__\n",
    "- NLP stands for natural language processing\n",
    "- Branch of AI that gives machine the ability to understand human language\n",
    "- human language can be in the form of text or audio format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Tokenization__\n",
    "- A technique involving dividing a sentence or phrase into smaller units known as __tokens__\n",
    "- Tokens can encompass words, dates, punctuation marks, or fragment of words\n",
    "- It is a critical step in many NLP tasks, such as text processing, language modelling, and machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of Tokenization__\n",
    "1. Word Tokenization\n",
    "    - In word tokenization, a text is divided into indivisual words\n",
    "    - Words are treated as basic unit of meaning\n",
    "    - __example:__ \n",
    "        - Input: \"Tokenization is an important NLP task.\"\n",
    "        - Output: [\"Tokenization\", \"is\", \"an\", \"important\", \"NLP\", \"task\", \".\"]\n",
    "2. Sentence Tokenization\n",
    "    - The text is segmented into sentences in sentence tokenization\n",
    "    - Useful for tasks requiring indivisual sentence analysis/processing\n",
    "    - __example__\n",
    "        - Input: \"Tokenization is an important NLP task. It helps break down text into smaller units.\"\n",
    "        - Output: [\"Tokenization is an important NLP task.\", \"It helps break down text into smaller units.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implementing Tokenization__\n",
    "1. Word Tokenization using word_tokenize()\n",
    "    - word_tokenize() function is used to break down a sentence into indivisual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# word_tokenize(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.', 'my', 'name', 'is', 'Asad', '.', 'I', 'am', '20', 'years', 'old']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"\"\"Hello. my name is Asad. I am 20 years old\"\"\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sentence tokenization using __sent_tokenize()__\n",
    "    - It is used to convert a segment of texts into list of sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax:\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, my name is Asad.', 'I am 20 years old']\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello, my name is Asad. I am 20 years old\"\n",
    "sentence = sent_tokenize(text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Stemming__\n",
    "- A method in text processing that eliminates prefixes and suffixes from words, transforming them into their fundamental/root form\n",
    "- It makes text simpler by using stemmers or stemming algorithms\n",
    "- __e.g:__ \"chocolates\" become \"chocolate\" and \"retrieval\" becomes \"retrieve\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stemmer in nltk(Using Porter's Stemmer)__\n",
    "- Based on idea that suffixes in English are made up of smaller and simpler suffixes\n",
    "- Group of stems is mapped to same stem and is not necessarily a meaningful word\n",
    "- __e.g:__ EED -> EE means  “if the word has at least one vowel and consonant plus EED ending, change the ending to EE” as ‘agreed’ becomes ‘agree’\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax:\n",
    "# from nltk.stem import PorterStemmer\n",
    "# porter=PorterStemmer()\n",
    "# porter.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "like\n",
      "like\n",
      "realiz\n"
     ]
    }
   ],
   "source": [
    "# Example 1:\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "print(porter.stem(\"liking\"))\n",
    "print(porter.stem(\"liked\"))\n",
    "print(porter.stem(\"likes\"))\n",
    "print(porter.stem(\"Realization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commun\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Stemmer giving wrong output\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "print(porter.stem(\"Communication\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Lemmetization__\n",
    "- Process of grouping together different inflected forms of a word so they can be analyzed as a single term\n",
    "- Similar to stemming, but it brings context to word\n",
    "- Links word with similar meaning to one word\n",
    "- __Note:__ Always convert text to lowercase before performing lemmization\n",
    "- __e.g:__\n",
    "    - meeting -> meet\n",
    "    - was -> be\n",
    "    - mice -> mouse\n",
    "    - better -> good\n",
    "    - corpora -> corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatizer in ntlk(Wordnet):__\n",
    "- It links words into sematic relations\n",
    "- It groups synonyms into in the form of synets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax:\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# WordNet=WordNetLemmatizer()\n",
    "# WordNet.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kite\n",
      "baby\n",
      "dogsflying\n",
      "smiling\n",
      "driving\n",
      "died\n",
      "tried\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "tokens = [\n",
    "    \"kites\",\n",
    "    \"babies\",\n",
    "    \"dogs\" \"flying\",\n",
    "    \"smiling\",\n",
    "    \"driving\",\n",
    "    \"died\",\n",
    "    \"tried\",\n",
    "    \"feet\",\n",
    "]\n",
    "for token in tokens:\n",
    "    print(wnl.lemmatize(token))\n",
    "\n",
    "# > kites ---> kite\n",
    "# > babies ---> baby\n",
    "# > dogs ---> dog\n",
    "# > flying ---> flying\n",
    "# > smiling ---> smiling\n",
    "# > driving ---> driving\n",
    "# > died ---> died\n",
    "# > tried ---> tried\n",
    "# > feet ---> foot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Stop Words__\n",
    "- A commonly used word (\"a\", \"the\", \"an\", or \"in\") that a search engine is programmed to ignore \n",
    "- We do not want these words to take up space in database or take up valuable processing time. For this reason, we remove them easily.\n",
    "- __e.g__\n",
    "    1. Can Listening be exhausting? -> Listening, exhausting\n",
    "    2. I like reading, so i read -> Like, reading, read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checking English Stop Words List__\n",
    "- It includes common words that carry little semantic meaning and are often exluded during text analysis\n",
    "- __e.g__ \"this\", \"and\", \"is\", \"for\", \"it\"\n",
    "- These are removed to focus more on meaningful terms when processing textdata in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Checking list of StopWords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing Stop Words using nltk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence, showing off the stop words filtration.\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words= stopwords.words('english')\n",
    "text= \"\"\"This is a sample sentence, showing off the stop words filtration.\"\"\"\n",
    "print(text)\n",
    "#Convert text into lowercase\n",
    "text=text.lower()\n",
    "#Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "filtered_sentence=[]\n",
    "\n",
    "#Filter out stop words\n",
    "for t in tokens:\n",
    "    if t not in stop_words:\n",
    "        filtered_sentence.append(t)\n",
    "#Print the filtered sentence\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Bag of Words Model__\n",
    "- A model used to preprocess the text by converting it into bag of words\n",
    "- Bag of words keeps a total count of occurences of most frequently used words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps for applying Bag of Words Model__\n",
    "1. __Preprocess the data__\n",
    "    - Convert text to lowercase\n",
    "    - Remove all non-word characters\n",
    "    - Remove all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beans ', 'i was trying to explain to somebody as we were flying in that s corn ', 'that s beans ', 'and they were very impressed at my agricultural knowledge ', 'please give it up for amaury once again for that outstanding introduction ', 'i have a bunch of good friends here today including somebody who i served with who is one of the finest senators in the country and we re lucky to have him your senator dick durbin is here ', 'i also noticed by the way former governor edgar here who i haven t seen in a long time and somehow he has not aged and i have ', 'and it s great to see you governor ', 'i want to thank president killeen and everybody at the u of i system for making it possible for me to be here today ', 'and i am deeply honored at the paul douglas award that is being given to me ', 'he is somebody who set the path for so much outstanding public service here in illinois ', 'now i want to start by addressing the elephant in the room ', 'i know people are still wondering why i didn t speak at the commencement ']\n"
     ]
    }
   ],
   "source": [
    "# Step 1:\n",
    "# Consider the following text:\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "text= \"Beans. I was trying to explain to somebody as we were flying in, that’s corn.  That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\n",
    "\n",
    "#Sentence tokenize\n",
    "tokens= nltk.sent_tokenize(text)\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=tokens[i].lower() #Convert text to lowercase\n",
    "    tokens[i]=re.sub(r'\\W',' ',tokens[i]) #Remove all non-word and punctuation characters\n",
    "    tokens[i]=re.sub(r'\\s+',' ',tokens[i]) # Remove all extra spaces\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain most frequent words in text\n",
    "    - Declare a dictionary to hold bag of words\n",
    "    - Tokenize each sentence into words\n",
    "    - For each word, check if it exists in the dictionary \n",
    "    - If it does, set count by 1, otherise increment that location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beans': 2, 'i': 12, 'was': 1, 'trying': 1, 'to': 8, 'explain': 1, 'somebody': 3, 'as': 1, 'we': 2, 'were': 2, 'flying': 1, 'in': 5, 'that': 4, 's': 3, 'corn': 1, 'and': 7, 'they': 1, 'very': 1, 'impressed': 1, 'at': 4, 'my': 1, 'agricultural': 1, 'knowledge': 1, 'please': 1, 'give': 1, 'it': 3, 'up': 1, 'for': 5, 'amaury': 1, 'once': 1, 'again': 1, 'outstanding': 2, 'introduction': 1, 'have': 3, 'a': 2, 'bunch': 1, 'of': 3, 'good': 1, 'friends': 1, 'here': 5, 'today': 2, 'including': 1, 'who': 4, 'served': 1, 'with': 1, 'is': 4, 'one': 1, 'the': 9, 'finest': 1, 'senators': 1, 'country': 1, 're': 1, 'lucky': 1, 'him': 1, 'your': 1, 'senator': 1, 'dick': 1, 'durbin': 1, 'also': 1, 'noticed': 1, 'by': 2, 'way': 1, 'former': 1, 'governor': 2, 'edgar': 1, 'haven': 1, 't': 2, 'seen': 1, 'long': 1, 'time': 1, 'somehow': 1, 'he': 2, 'has': 1, 'not': 1, 'aged': 1, 'great': 1, 'see': 1, 'you': 1, 'want': 2, 'thank': 1, 'president': 1, 'killeen': 1, 'everybody': 1, 'u': 1, 'system': 1, 'making': 1, 'possible': 1, 'me': 2, 'be': 1, 'am': 1, 'deeply': 1, 'honored': 1, 'paul': 1, 'douglas': 1, 'award': 1, 'being': 1, 'given': 1, 'set': 1, 'path': 1, 'so': 1, 'much': 1, 'public': 1, 'service': 1, 'illinois': 1, 'now': 1, 'start': 1, 'addressing': 1, 'elephant': 1, 'room': 1, 'know': 1, 'people': 1, 'are': 1, 'still': 1, 'wondering': 1, 'why': 1, 'didn': 1, 'speak': 1, 'commencement': 1}\n",
      "['i', 'the', 'to', 'and', 'in', 'for', 'here', 'that', 'at', 'who', 'is', 'somebody', 's', 'it', 'have', 'of', 'beans', 'we', 'were', 'outstanding', 'a', 'today', 'by', 'governor', 't', 'he', 'want', 'me', 'was', 'trying', 'explain', 'as', 'flying', 'corn', 'they', 'very', 'impressed', 'my', 'agricultural', 'knowledge', 'please', 'give', 'up', 'amaury', 'once', 'again', 'introduction', 'bunch', 'good', 'friends', 'including', 'served', 'with', 'one', 'finest', 'senators', 'country', 're', 'lucky', 'him', 'your', 'senator', 'dick', 'durbin', 'also', 'noticed', 'way', 'former', 'edgar', 'haven', 'seen', 'long', 'time', 'somehow', 'has', 'not', 'aged', 'great', 'see', 'you', 'thank', 'president', 'killeen', 'everybody', 'u', 'system', 'making', 'possible', 'be', 'am', 'deeply', 'honored', 'paul', 'douglas', 'award', 'being', 'given', 'set', 'path', 'so']\n"
     ]
    }
   ],
   "source": [
    "# Step 1:\n",
    "# Consider the following text:\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "text= \"Beans. I was trying to explain to somebody as we were flying in, that’s corn.  That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\n",
    "\n",
    "#Sentence tokenize\n",
    "tokens=sent_tokenize(text)\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=tokens[i].lower() #Convert text to lowercase\n",
    "    tokens[i]=re.sub(r'\\W',' ',tokens[i]) #Remove all non-word characters\n",
    "    tokens[i]=re.sub(r'\\s+',' ',tokens[i]) # Remove all punctuations\n",
    "    \n",
    "# Step 2:\n",
    "#Tokenize sentence into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_freq={}\n",
    "for token in tokens:\n",
    "    words=word_tokenize(token)\n",
    "    for word in words:\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq[word]=1\n",
    "        else:\n",
    "            word_freq[word]+=1\n",
    "print(word_freq)\n",
    "\n",
    "#Create a heap of most frequent 100 words\n",
    "import heapq\n",
    "word_freq= heapq.nlargest(100,word_freq,key=word_freq.get)\n",
    "print(word_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Building bag of words model\n",
    "    - Construct a list of whether the word in each sentence is a frequent word or not\n",
    "    - If a word is frequent, set it as 1, otherwise set it as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1:\n",
    "# Consider the following text:\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "text= \"Beans. I was trying to explain to somebody as we were flying in, that’s corn.  That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\n",
    "\n",
    "#Sentence tokenize\n",
    "tokens=sent_tokenize(text)\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=tokens[i].lower() #Convert text to lowercase\n",
    "    tokens[i]=re.sub(r'\\W',' ',tokens[i]) #Remove all non-word characters\n",
    "    tokens[i]=re.sub(r'\\s+',' ',tokens[i]) # Remove all punctuations\n",
    "# Step 2:\n",
    "#Tokenize sentence into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_freq={}\n",
    "for token in tokens:\n",
    "    words=word_tokenize(token)\n",
    "    for word in words:\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq[word]=1\n",
    "        else:\n",
    "            word_freq[word]+=1\n",
    "#Create a heap of most frequent 100 words\n",
    "import heapq\n",
    "word_freq= heapq.nlargest(100,word_freq,key=word_freq.get)\n",
    "#Step 3:\n",
    "x = []\n",
    "for token in tokens:\n",
    "    vector = []\n",
    "    for word in word_freq:\n",
    "        if word in word_tokenize(token):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    x.append(vector)\n",
    "x=np.array(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __TF-IDF__\n",
    "- It stands for Term Frequency Inverse Document Frequency of records\n",
    "- It is defined as calculation of how relevant in a word is in a series or corpus is to a text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Terminologies in TF-IDF__\n",
    "1. __Term Frequency__\n",
    "    - In a document d, term frequency represents number of instances of a word t\n",
    "    - The weight of a term that occurs in a document is propotional to term frequency\n",
    "    - __tf(t,d)= count of t in d/ number of words in d__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Document Frequency__\n",
    "    - This tests meaning of text (similar to TF)\n",
    "    - The only difference is that in document d, TF is the frequency counter for term t, while df is the number of occurences in the document set N of term t\n",
    "    - the number of papers in which the word is present is DF\n",
    "    - __df(t)=occurence of t in documents__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Inverse Document Frequency__\n",
    "    - It tests how relevant a word is\n",
    "    - Aim of search is to  locate appropriate records that fit the demand\n",
    "    - Since tf considers all term equally signifigant\n",
    "    - __idf(t)=log(N/df(t))__ where\n",
    "        - __df(t)= Document frequency of term t__\n",
    "        - __N(t)= Number of documents containing term t__\n",
    "        - __N is number of documents__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __TF-IDF__\n",
    "    - __TF-IDF(t,d,D)=TF(t,d)*IDF(t,D)__\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 TF-IDF values:\n",
      "  and: 0.0000\n",
      "  blue: 0.6031\n",
      "  bright: 0.0000\n",
      "  can: 0.0000\n",
      "  in: 0.0000\n",
      "  is: 0.4883\n",
      "  see: 0.0000\n",
      "  shining: 0.0000\n",
      "  sky: 0.4883\n",
      "  sun: 0.0000\n",
      "  the: 0.3992\n",
      "  we: 0.0000\n",
      "Document 2 TF-IDF values:\n",
      "  and: 0.4240\n",
      "  blue: 0.3343\n",
      "  bright: 0.2706\n",
      "  can: 0.0000\n",
      "  in: 0.0000\n",
      "  is: 0.5413\n",
      "  see: 0.0000\n",
      "  shining: 0.0000\n",
      "  sky: 0.2706\n",
      "  sun: 0.2706\n",
      "  the: 0.4425\n",
      "  we: 0.0000\n",
      "Document 3 TF-IDF values:\n",
      "  and: 0.0000\n",
      "  blue: 0.0000\n",
      "  bright: 0.3310\n",
      "  can: 0.0000\n",
      "  in: 0.5186\n",
      "  is: 0.3310\n",
      "  see: 0.0000\n",
      "  shining: 0.0000\n",
      "  sky: 0.3310\n",
      "  sun: 0.3310\n",
      "  the: 0.5412\n",
      "  we: 0.0000\n",
      "Document 4 TF-IDF values:\n",
      "  and: 0.0000\n",
      "  blue: 0.0000\n",
      "  bright: 0.2391\n",
      "  can: 0.3746\n",
      "  in: 0.0000\n",
      "  is: 0.0000\n",
      "  see: 0.3746\n",
      "  shining: 0.3746\n",
      "  sky: 0.0000\n",
      "  sun: 0.4782\n",
      "  the: 0.3910\n",
      "  we: 0.3746\n"
     ]
    }
   ],
   "source": [
    "# Example 1:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example corpus of documents\n",
    "corpus = [\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright and the sky is blue\",\n",
    "    \"The sun in the sky is bright\",\n",
    "    \"We can see the shining sun, the bright sun\"\n",
    "]\n",
    "# Step 2\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Get TF-IDF values \n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (i.e., the words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 3\n",
    "# Convert the sparse matrix to a dense format for display\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "\n",
    "# Step 4\n",
    "# Display the results\n",
    "for i, doc in enumerate(dense_matrix):\n",
    "    print(f\"Document {i+1} TF-IDF values:\")\n",
    "    for j, score in enumerate(doc.tolist()[0]):\n",
    "        print(f\"  {feature_names[j]}: {score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Word Embeddings__\n",
    "- Method of extracting features out of textso that we can input those text into machine learning model to work with text data\n",
    "- The preserve syntactical and semantic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Word2vec__\n",
    "- NLP technique for obtaining vector representation of words\n",
    "- They capture meaning of words based on surrounding words\n",
    "- The semantic information and relation between different words is preserved\n",
    "- Developed by researchers at Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating Word2Vec__\n",
    "1. Tokenize the sentences\n",
    "2. Train a word2vec model on tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\S.A\n",
      "[nltk_data]     Tech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      "[ 9.9660458e-05  3.0821718e-03 -6.8126968e-03 -1.3774707e-03\n",
      "  7.6670009e-03  7.3487638e-03 -3.6748096e-03  2.6433435e-03\n",
      " -8.3152140e-03  6.2032156e-03 -4.6383245e-03 -3.1595279e-03\n",
      "  9.3136756e-03  8.6830143e-04  7.4923071e-03 -6.0711433e-03\n",
      "  5.1649613e-03  9.9231759e-03 -8.4552281e-03 -5.1386035e-03\n",
      " -7.0631793e-03 -4.8644664e-03 -3.7822647e-03 -8.5424874e-03\n",
      "  7.9531269e-03 -4.8400508e-03  8.4222108e-03  5.2616741e-03\n",
      " -6.5445201e-03  3.9631235e-03  5.4638721e-03 -7.4266698e-03\n",
      " -7.4060163e-03 -2.4757332e-03 -8.6282277e-03 -1.5768069e-03\n",
      " -4.0439886e-04  3.2994591e-03  1.4390461e-03 -8.8006386e-04\n",
      " -5.5882698e-03  1.7333244e-03 -8.9027028e-04  6.7976406e-03\n",
      "  3.9730081e-03  4.5287893e-03  1.4353305e-03 -2.7041151e-03\n",
      " -4.3698736e-03 -1.0341319e-03  1.4363434e-03 -2.6442122e-03\n",
      " -7.0785680e-03 -7.8077246e-03 -9.1245631e-03 -5.9377626e-03\n",
      " -1.8496348e-03 -4.3226960e-03 -6.4599686e-03 -3.7209135e-03\n",
      "  4.2887270e-03 -3.7411784e-03  8.3785607e-03  1.5363722e-03\n",
      " -7.2379941e-03  9.4376719e-03  7.6372884e-03  5.4892199e-03\n",
      " -6.8521285e-03  5.8181966e-03  4.0036314e-03  5.1798988e-03\n",
      "  4.2536142e-03  1.9410899e-03 -3.1704542e-03  8.3553176e-03\n",
      "  9.6138157e-03  3.7964704e-03 -2.8379683e-03  1.1605529e-05\n",
      "  1.2177721e-03 -8.4549012e-03 -8.2232114e-03 -2.3262552e-04\n",
      "  1.2371656e-03 -5.7410691e-03 -4.7200238e-03 -7.3430403e-03\n",
      "  8.3243037e-03  1.1728019e-04 -4.5111864e-03  5.7020700e-03\n",
      "  9.1784708e-03 -4.0956256e-03  7.9612657e-03  5.3680264e-03\n",
      "  5.8741397e-03  5.1875552e-04  8.2144458e-03 -7.0121824e-03]\n",
      "\n",
      "Words most similar to 'learning':\n",
      "[('fields', 0.15016479790210724), ('enjoy', 0.14901481568813324), ('love', 0.12813477218151093)]\n",
      "\n",
      "Similarity between 'learning' and 'processing': -0.1353098452091217\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample sentences to train the Word2Vec model\n",
    "sentences = [\n",
    "    \"Hello, my name is Asad.\",\n",
    "    \"I love natural language processing.\",\n",
    "    \"Word2Vec is a powerful model for word embeddings.\",\n",
    "    \"I enjoy learning new things.\",\n",
    "    \"Deep learning is fascinating.\",\n",
    "    \"Machine learning and artificial intelligence are interrelated fields.\"\n",
    "]\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized sentences\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Example usage: Find the vector of a word\n",
    "word = \"learning\"\n",
    "if word in model.wv:\n",
    "    print(f\"Vector for '{word}':\\n{model.wv[word]}\\n\")\n",
    "\n",
    "# Example usage: Find most similar words to a given word\n",
    "similar_words = model.wv.most_similar(\"language\", topn=3)\n",
    "print(f\"Words most similar to 'learning':\\n{similar_words}\\n\")\n",
    "\n",
    "# Example usage: Find similarity between two words\n",
    "similarity_score = model.wv.similarity(\"learning\", \"processing\")\n",
    "print(f\"Similarity between 'learning' and 'processing': {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2:\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # Download the tokenizer models if not already downloaded\n",
    "\n",
    "sample = \"Word embeddings are dense vector representations of words.\"\n",
    "tokenized_corpus = word_tokenize(sample.lower()) # Lowercasing for consistency\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus],\n",
    "\t\t\t\t\t\tvector_size=100, # Dimensionality of the word vectors\n",
    "\t\t\t\t\t\twindow=5,\t\t # Maximum distance between the current and predicted word within a sentence\n",
    "\t\t\t\t\t\tsg=1,\t\t\t # Skip-Gram model (1 for Skip-Gram, 0 for CBOW)\n",
    "\t\t\t\t\t\tmin_count=1,\t # Ignores all words with a total frequency lower than this\n",
    "\t\t\t\t\t\tworkers=4)\t # Number of CPU cores to use for training the model\n",
    "\n",
    "# Training\n",
    "skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\n",
    "skipgram_model.save(\"skipgram_model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "vector_representation = loaded_model.wv['word']\n",
    "print(\"Vector representation of 'word':\", vector_representation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
